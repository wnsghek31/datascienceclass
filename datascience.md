## 상관계수

### 1단계: 변수 사이의 선형 관계 조사(Pearson)


* 상관 계수만을 기초로 한변수의 변화가 다른 변수의 변화를 유발한다는 결론을 내리는것은 적절하지않음 . 적절히 통제된 실험에서만 인과 관계를 확인할수있다.
* Pearson 상관 계수는 극단 데이터 값의 영향을 상당히 많이 받음. 데이터 집합에 나머지 값들과 매우 다른 값이 하나 있으면 상관계수의 값이 크게 변경됨. 다라서 극단값의 원인을 식별해야해. 모든 데이터 입력 또는 측정 오류를 수정한다. 비정상적인 일회성 사건과 연관된 데이터 값을 삭제한다. 그런다음 분석 반복
* 낮은 Pearson 상관 계수는 변수 사이에 관계가 없다는것을 의미하는것이아니다. 변수 사이에 비선형 관계가 있을수도있다. 비선형 관계를 그래픽으로 확인하려면 산점도를 생성하거나 적합선 그림을 사용하라.

### 2단계: 상관 계수가 유의한지 여부 확인

변수 사이에 유의한 상관 관계가 있는지 여부를 확인하려면 p-값을 유의 수준과 비교해야한다. 일반적으로 0.05의 유의수준(a)이 좋다. 0.05의 α는 실제로 상관 관계가 존재하지 않는데 상관 관계가 존재한다는 결론을 내릴 위험이 5%라는 것을 나타냅니다

* p-값 ≤ α: 상관 관계가 통계적으로 유의합니다.
	p-값이 유의 수준보다 작거나 같으면 상관 계수가 0과 다르다는 결론을 내릴 수 있습니다.
    
* p-값 > α: 상관 관계가 통계적으로 유의하지 않습니다.
	p-값이 유의 수준보다 크면 상관 관계가 0과 다르다는 결론을 내릴 수 없습니다.
    


## 확률 분포
확률변수 x가 특정한 값을 가질 확률을 나타내는 함수
> 예를 들어서 두 개의 주사위를 던졌을때, 나오는 눈의 합이 x가 될 확률을 정의하는 것이 확률 분포이다. (2가 될 확률 2%,3이 될 확률 4%....)

### 확률 변수
확률 변수가 주사위의 숫자와 같은 이산값을 취할때 , **이산확률변수**
키나 몸무게처럼 실수로 연속된 무한한값을 취할때 , **연속확률변수**

### 확률 밀도 함수 
연속 확률 변수에서 확률 변수의 분포를 나타내는 그래프
45~60 적분하면 45~60일 확률이 나오는것

### 확률 질량 함수
이산 확률 변수에서 특정값에 대한 확률을 나타내는 함수.


![](https://github.com/wnsghek31/datascienceclass/blob/master/gyul.PNG)

### 결합 확률 분포
두개 이상의 확률변수가 관련된 확률분포
두 변수간의 결합확률분포는 X와 Y가 만나는 각각의 값
x=3% and y=10% -> 0.2 

학생의 키와 몸무게를 측정하면 한 학생당 두 개의 자료 ( x ,  y )가 한 쌍으로 나오게 된다. 이렇게 취득한 자료를 확률 변수  X 와  Y 로 볼 때, 이들의 확률 분포를 한번에 묘사하기 위한 확률 분포를 결합 확률 분포(joint probability distribution)라고 한다.


### 주변 확률 분포
X와 Y의 결합분포에서 X 또는 Y의 어느 하나만의 확률 분포를 말함
결합 확률 분포의 주변에 표시되기 때문에 이를 주변 확률 분포라고함
> 예를 들어 위 표를 기준으로 P(X = 5%)의 주변확률분포를 알고 싶다면  Y가 5%일 때 X=5% 인 값 0.1과 Y=10%인 값 0.1, Y=20%인 값 0.2를 모두 합산한 금액 0.4가 X=5%일 때 주변확률분포가 되는 것

이것은 기댓값을 구할때 사용되는데, 
X값의 기댓값을 구할 경우에는 X가 3%일대 X의 주변확률 0.5를 곱하고, X가 5%일때 주변확률 0.4를 곱하고, x가 10%일때 주변확률 0.1을 곱한값을 더해주면 E(X)가 산정됨
> E(X) = 4.5% 
	마찬가지로 E(Y) = 5x0.2 + 10x0.3 + 20x0.5 = 14%


### 다항 분포
> 예를 들어 주사위를 던져 어떤 눈금이 나오는지를 실험한다고 하자 , 이때 베르누이 분포는 "눈금 3이 나올 확률" 처럼 하나의 눈금 밖에 실험을 못해 그리고 이항 분포도 "눈금 3이 네번 나올 확률"처럼 횟수가 추가되기는 하지만 역시 하나의 눈금밖에 실험을 못해 하지만 다항 분포는 "눈금2가 한번 , 눈금3이 네번 그리고 눈금 6이 두번 나올 확률"처럼 여러가지 눈금이 나올 상황을 실험할수있다.


사건의 전후관계가 바꼈을때 두사건사이에 관계를 정의한식
### 베이즈 정리
기존 통계학은 모집단을 변하지 않는 대상으로 규정하지만 베이즈 통계학에서는 모집단을 미리 확정짓지 않는것이 특징
베이즈 정리는 이전의 경험과 현재의 증거를 토대로 어떤 사건의 확률을 추론함
베이즈 정리는 P(A|B) 를 알고잇을때, 관계가 정반대인 P(B|A)를 계산하기위해 나타남

베이지언 확률은 사후확률(posterior probability)을 사전확률(prior probability)과 likelihood를 이용해서 계산할 수 있도록 해 주는 확률 변환식

P(A) : A의 사전확률(evidence) - 현재의 증거

P(B) : B의 사전확률(prior probability) - 과거의 경험
☞ 사전확률(prior probability): p(x), 관측자가 관측을 하기 전에 시스템 또는 모델에 대해 가지고 있는 선험적 확률. 예를 들어, 남여의 구성비를 나타내는 p(남자), p(여자) 등이 사전확률에 해당한다.

P(A|B) : 사건 B가 주어졌을 때 A의 조건부 확률(likelihood) - 알려진 결과에 기초한 어떤 가설에 대한 가능성 (즉, 이 가설을 지지하는 정도)
☞ likelihood: p(z|x), 어떤 모델에서 해당 데이터(관측값)이 나올 확률

P(B|A) : 사건 A라는 증거에 대한 사후 확률(posterior probability) - 사건 A가 일어났다는 것을 알고 , 그것이 사건 B로 부터 일어난 것이라고 생각되는 조건부 확률
☞ 사후확률(posterior probability): p(x|z), 사건이 발생한 후(관측이 진행된 후) 그 사건이 특정 모델에서 발생했을 확률


![](https://github.com/wnsghek31/datascienceclass/blob/master/bay.PNG)





### MLE 과 MAP
####MLE 
최우도추정 (Maximum Likelihood Estimation)
Choose value that maximizes the probability of observed data

####MAP
최대사후확률
Choose value that is most probable given observed data and prior belief

관측값을 z, 그 값이 나온 클래스(또는 모델)를 x라 하자. 예를 들어, 바닥에 떨어진 머리카락의 길이(z)를 보고 그 머리카락이 남자 것인지 여자 것인지 성별(x)을 판단하는  문제를 생각해 보자

* ML(Maximum Likelihood) 방법: ML 방법은 남자에게서 그러한 머리카락이 나올 확률 p(z|남)과 여자에게서 그러한 머리카락이 나올 확률 p(z|여)을 비교해서 가장 확률이 큰, 즉 likelihood가 가장 큰 클래스(성별)를 선택하는 방법이다.
* MAP(Maximum A Posteriori) 방법: MAP 방법은 z라는 머리카락이 발견되었는데 그것이 남자것일 확률 p(남|z), 그것이 여자것일 확률 p(여|z)를 비교해서 둘 중 큰 값을 갖는 클래스(성별)를 선택하는 방법이다. 즉, 사후확률(posterior prabability)를 최대화시키는 방법으로서 MAP에서 사후확률을 계산할 때 베이즈 정리가 이용된다.

ML과 MAP의 차이는 남녀의 성비를 고려하면 명확해짐 만일 인구의 90%가 남자고 여자는 10% 밖에 없다고 하자. **ML은 남녀의 성비는 완전히 무시하고 순수하게 남자중에서 해당 길이의 머리카락을 가질 확률, 여자중에서 해당 길이의 머리카락을 가질 확률만을 비교하는 것**이고 , ** MAP는 각각의 성에서 해당 머리카락이 나올 확률 뿐만 아니라 남녀의 성비까지 고려하여 최종 클래스를 결정하는 방법**

![](https://github.com/wnsghek31/datascienceclass/blob/master/MAP.PNG)


ML보다는 MAP 방법이 보다 정확한 classification 방법임을 알 수 있다. 하지만 많은 경우, 사전확률(prior probability)인 p(남), p(여)를 모르는 경우가 대부분이기 때문에 단순하게 p(남) = p(여)로 놓고 문제를 푸는 경우가 많은데, 이 경우 MAP는 ML과 같게 된다.



## 가설검정

1. **가설을 세우는것** 
	일반적으로 귀무가설을 세운 뒤, 표본을 통해 얻은 자료를 바탕으로 귀무가설이 존재할 확률에 따라 기각을 할것인지 수용을 할것인지를 결정하게된다. 귀무가설이 기각 될때 받아들여지는 가설이 대립가설
	> 귀무가설이 옳을때 기각한다면 1종오류 , 귀무가설이 실제로 옳지 않을때(대립가설이 옳을때) 귀무가설을 기각하지 않는다면 2종 오류
	 
2. **유의수준과 임계치의 결정(a값)**
	결론을 잘못 내릴 가능성을 어느 정도 수준까지 허용할 것인가를 연구자가 	주관적으로 결정해야 한다. 즉, 통계적 유의 수준(알파)을 결정해야 한다.

3. 귀무가설 채택영역과 기각영역 설정 (p-value , c-value)
4. 통계량계산
5. 통계량과 임계치의 비교 및 결론(P가 a보다 작으면 H1채택 / Z*가 Z보다 크면 H1채택)


### 검정력
검정력은 간단히 말해, 2종 오류를 범하지 않을 확률이다. 즉 대립가설이 옳음에도 귀무가설을 기각하지 않는 오류가 2종 오류(β)인데, 이러한 오류를 범하지 않을 확률 1-β를 검정력(power)
어떠한 차이가 존재하고 있다는 가설이 옳을 때 정확히 유의차가 있다고 말할 수 있는 확률. 즉 검정력이 높을수록 통계적으로 내린 결론이 확률상 옳다는 것이다. 
**가설 검정에서 검정력은 귀무가설을 기각하여 옳은 결정을내릴 수 있는 확률. 이것이 2종 오류를 범하지않을 확률이랑 같은말이지	 **
두 종류의 오류를 동시에 줄일수 없으므로 1종 오류를 고정하고 검정력을 최대화 하는 검정방법을 탐색



### eigenvalue , eigenvector

행렬 A를 선형변환으로 봤을때 , 선형변환 A에 의한 변환 결과가 자기 자신의 상수배가 되는0이 아닌 벡터를 eigenvector 라 하고 이 상수배 값을 eigenvalue라고한다.
n x n 정방행렬(고유값, 고유벡터는 정방행렬에 대해서만 정의된다) A에 대해 Av = λv를 만족하는 0이 아닌 열벡터 v를 고유벡터, 상수 λ를 고유값이라 정의


### PCA (Principle Component Analysis)
PCA는 분포된 데이터들의 주성분(Principal Component)를 찾아주는 방법

![](https://github.com/wnsghek31/datascienceclass/blob/master/pca.PNG)


이 데이터들의 분포 특성을 2개의 벡터로 가장 잘 설명할 수 있는 방법은 무엇일까? 그건 바로, 그림에서와 같이 e1, e2 두 개의 벡터로 데이터 분포를 설명하는 것이다. e1의 방향과 크기, 그리고 e2의 방향과 크기를 알면 이 데이터 분포가 어떤 형태인지를 가장 단순하면서도 효과적으로 파악할 수 있다.
PCA는 데이터 하나하나에 대한 성분을 분석하는게 아니라 여러 데이터들이 모여 하나의 분포를 이룰때 이 분포의 주성분을 분석하는 방법
**여기서 주성분이라 함은 그 방향으로 데이터들의 분산이 가장 큰 방향벡터를 의미**

> PCA는 2차원 데이터 집합에 대해 PCA를 수행하면 2개의 서로 수직인 주성분 벡터를 반환하고, 3차원 점들에 대해 PCA를 수행하면 3개의 서로 수직인 주성분 벡터들을 반환

#### eigenface와 영상인식 응용
PCA가 영상인식에 활용되는 대표적인 예는 얼굴인식(face recognition) 그리고 관련된 개념 혹은 용어로서 eigenface(아이겐페이스)가 있다.

> 20개의 45x40 얼굴 이미지들이 있다고 하자
	얼굴 이미지는 45x40 = 1800 차원의 벡터로 생각할수있다. (즉 각각의 이미지는 1800 차원 공간에서 한점에 대응)
    이제 이 20개의 1,800차원 점 데이터들을 가지고 PCA를 수행하면 데이터의 차원 수와 동일한 개수의 주성분 벡터들을 얻을 수 있다
    이렇게 얻어진 주성분 벡터들을 다시 이미지로 해석한 것이 eigenface.
    eigenface들은 데이터들에 공통된 요소(얼굴의 전반적인 형태)를 나타내고 뒤로 갈수록 세부적인 차이 정보를 나타낸다. 그리고 더 뒤로 가면 거의 노이즈(noise)성 정보를 나타낸다.
     많은 수의 eigenface를 이용하면 원본 얼굴과 거의 유사한 근사(복원) 결과를 볼 수 있지만 k가 작아질수록 개인 고유의 얼굴 특성은 사라지고 공통의 얼굴 특성이 남게 된다 

##### PCA 계산
PCA를 알기위해서는 공분산 행렬을 알아야한다
x와 y의 공분산은
(공분산 사진)

x의 분산은 x들이 평균을 중심으로 얼마나 흩어져있는지, 
x와y의 공분산은 x,y의 흩어진 정도가 얼마나 서로 상관관계를 가지고 흩어졌는지, 
> 예를 들어, x와 y 각각의 분산은 일정한데 x가 mx보다 클때 y도 my보다 크면 공분산은 최대가 되고, x가 mx보다 커질때 y는 my보다 작아지면 공분산은 최소(음수가 됨), 서로 상관관계가 없으면 공분산은 0

**공분산 행렬(covariance matrix)**이란 데이터의 좌표 성분들 사이의 공분산 값을 원소로 하는 행렬로서 데이터의 i번째 좌표 성분과 j번째 좌표 성분의 공분산 값을 행렬의 i행 j열 원소값으로 하는 행렬

PCA란 한마다로 말하면 입력 데이터들의 공분산 행렬(covariance matrix)에 대한 고유값분해(eigendecomposition)로 볼 수 있음
이 때 나오는 고유벡터가 주성분 벡터로서 데이터의 분포에서 분산이 큰 방향을 나타내고, 대응되는 고유값(eigenvalue)이 그 분산의 크기를 나타냄
첫번째 주성분 벡터 e1은 데이터들의 분산이 가장 큰 방향을 나타낸

![](https://github.com/wnsghek31/datascienceclass/blob/master/pca.PNG)



#### SVD (Singular Value Decomposition)

특이값 분해(SVD)는 고유값 분해(eigendecomposition)처럼 행렬을 대각화하는 한 방법
 특이값 분해가 유용한 이유는 행렬이 정방행렬이든 아니든 관계없이 모든 m x n 행렬에 대해 적용 가능하기 때문 (고유값 분해(EVD)는 정방행렬에 대해서만 적용 가능하며 또한 정방행렬 중에서도 일부 행렬에 대해서만 적용 가능한 대각화 방법)
![](https://github.com/wnsghek31/datascienceclass/blob/master/svd.PNG)


U는 AA^T를 고유값분해 해서 얻어진 직교행렬로 U의 열벡터들을 A의 left singular vector라부른다.
V는 A^TA를 고유값분해해서 얻어진 직교행렬로서 V 의 열벡터들을 A의 right singular vector
 Σ는 AA^T, A^TA를 고유값분해해서 나오는 고유값(eigenvalue)들의 square root(제곱근)를 대각원소로 하는 m x n	 직사각 대각행렬로 그 대각원소들을 A의 특이값(singular value)이라 부른다

 